apiVersion: kafka.banzaicloud.io/v1beta1
kind: KafkaCluster
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
  name: kafka
spec:
  # Specify if the cluster should use headlessService for Kafka or individual services
  # using service/broker may come in handy in case of service mesh
  headlessServiceEnabled: true
  # Specify the usable ingress controller, only envoy and istioingress supported can be left blank
  ingressController: "envoy"
  # Specify the zookeeper addresses where the Kafka should store it's metadata
  zkAddresses:
    - "zookeeper-client.zookeeper:2181"
  # Specify the zookeeper path where the Kafka related metadatas should be placed
  # By default it is bound to "/" and can be left blank
  zkPath: "/kafka"
  # rackAwareness add support for Kafka rack aware feature
  rackAwareness:
    # operator will use these labels from the nodes to create the rack for Kafka
    labels:
      - "failure-domain.beta.kubernetes.io/region"
      - "failure-domain.beta.kubernetes.io/zone"
  # oneBrokerPerNode if set to true every broker is started on a new node, if there is not enough node to do that
  # it will stay in pending state. If set to false the operator also tries to schedule the brokers to a unique node
  # but if the node number is insufficient the brokers will be scheduled to a node where a broker is already running.
  oneBrokerPerNode: false
  # Specify the Kafka Broker related settings
  # clusterImage can specify the whole kafkacluster image in one place
  #clusterImage: "ghcr.io/banzaicloud/kafka:2.13-3.1.0
  # readOnlyConfig specifies the read-only type kafka config cluster wide, all these will be merged with broker specified
  # readOnly configurations, so it can be overwritten per broker.
  #clusterWideConfig specifies the cluster-wide kafka config cluster wide, all these can be overridden per-broker
  #clusterWideConfig: |
  #  background.threads=10
  #readOnlyConfig: |
  #  auto.create.topics.enable=false
  # brokerConfigGroups specifies multiple broker configs with unique name
  #rollingUpgradeConfig specifies the rolling upgrade config for the cluster
  #rollingUpgradeConfig:
  #failureThreshold controls how many failures the cluster can tolerate during a rolling upgrade. Once the number of
  # failures reaches this threshold a rolling upgrade flow stops. The number of failures is computed as the sum of
  #	distinct broker replicas with either offline replicas or out of sync replicas and the number of alerts triggered by
  #	alerts with 'rollingupgrade'
  #  failureThreshold: 1
  brokerConfigGroups:
    # Specify desired group name (eg., 'default_group')
    default_group:
      # all the brokerConfig settings are available here
      storageConfigs:
        - mountPath: "/kafka-logs"
          pvcSpec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 10Gi
        #- mountPath: "/ephemeral-dir1"
        #  emptyDir:
        #    sizeLimit: 100Mi
      # Add custom labels to broker pods within the config group
      # brokerLabels:
      #   kafka_broker_group: "default_group"
  # All Broker requires an image, unique id, and storageConfigs settings
  brokers:
      # Unique broker id which is used as kafka config broker.id
    - id: 0
      # brokerConfigGroup can be used to ease the broker configuration, if set no only the id is required
      #brokerConfigGroup: "default_group"
      # readOnlyConfig can be used to pass Kafka config https://kafka.apache.org/documentation/#brokerconfigs
      # which has type read-only these config changes will trigger rolling upgrade
      readOnlyConfig: |
        auto.create.topics.enable=false
        cruise.control.metrics.topic.auto.create=true
        cruise.control.metrics.topic.num.partitions=1
        cruise.control.metrics.topic.replication.factor=2
      brokerConfig:
        # Docker image used by the operator to create the Broker with id 0
        #image: "ghcr.io/banzaicloud/kafka:2.13-3.1.0
        # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests
        # through this property
        #resourceRequirements:
        #  limits:
        #    memory: "300Mi"
        #    cpu: "200m"
        #  requests:
        #    memory: "300Mi"
        #    cpu: "200m"
        # nodeAffinity can be specified, operator populates this value if new pvc added later to brokers
        # nodeAffinity:
        # nodeSelector can be specified, which set the pod to fit on a node
        # nodeSelector:
        # tolerations can be specified, which set the pod's tolerations
        # tolerations:
        # config parameter can be used to pass Kafka config https://kafka.apache.org/documentation/#brokerconfigs
        # which has type per-broker
        config: |
          sasl.enabled.mechanisms=PLAIN
        # serviceAccountName specifies the serviceAccount used for this specific broker
        #serviceAccountName: "kafka"
        # imagePullSecrets specifies the secret to use when using private registry
        #imagePullSecrets: "k8ssecret"
        # kafkaHeapOpts specifies the jvm heap size for the broker
        #kafkaHeapOpts: "-Xmx4G -Xms4G"
        # kafkaJvmPerfOpts specifies the jvm performance configs for the broker
        #kafkaJvmPerfOpts: "-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true -Dsun.net.inetaddr.ttl=60"
        # storageConfigs specifies the broker log related configs
        storageConfigs:
          # mountPath will be used in kafka config log.dirs so it must be unique
          - mountPath: "/kafka-logs"
            # pvcSpec describes the PVC used for the mountPath described above
            # it requires a kubernetes PVC spec
            pvcSpec:
              accessModes:
                - ReadWriteOnce
              # storageClassName: standard
              resources:
                requests:
                  storage: 10Gi
          #- mountPath: "/kafka-second-log-volume"
          #    pvcSpec:
          #      accessModes:
          #        - ReadWriteOnce
          #      resources:
          #        requests:
          #          storage: 3Gi
        # Define an extra initContainer and volume that copies a jar form a custom image to the broker container
        #initContainers:
        #  - name: "copy-jar"
        #    image: "custom-image:latest"
        #    command: "cp"
        #    args: ["/path/to/jar", "/jars/"]
        #    volumeMounts:
        #      - mountPath: "/jars"
        #        name: "jars"
        #volumes:
        #  - name: "jars"
        #    emptyDir: {}
        #volumeMounts:
        #  - mountPath: "/opt/kafka/libs/extra-jars"
        #    name: "jars"
    - image: "ghcr.io/banzaicloud/kafka:2.13-3.1.0"
      id: 1
      config: |
        auto.create.topics.enable=false
      storageConfigs:
        - mountPath: "/kafka-logs"
          pvcSpec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 10Gi
      brokerConfig:
        # BrokerIngressMapping allows to set specific ingress to a specific broker mappings.
        # If left empty, all broker will inherit the default one specified under external listeners config
        # Only used when ExternalListeners.Config is populated
        brokerIngressMapping:
          - " ingress-az1"
    - id: 2
      readOnlyConfig: |
        auto.create.topics.enable=false
      brokerConfig:
        image: "ghcr.io/banzaicloud/kafka:2.13-3.1.0"
        storageConfigs:
          - mountPath: "/kafka-logs"
            pvcSpec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
  # listenersConfig specifies kafka's listener specific configs
  listenersConfig:
    # externalListeners specifies settings required to access kafka externally
    externalListeners:
      # type defines the used security type ssl, plaintext, sasl_plaintext, sasl_ssl
      - type: "ssl"
        # Kafka enables to name your listeners
        name: "external"
        # Operator uses a single LoadBalancer and an Envoy proxy to enable external access
        # to differentiate between brokers we are using ports, with externalStartingPort
        # the user can specify the starting port where the first broker will be available
        externalStartingPort: 19090
        # containerPort describes what port should be used by the broker to handle the communication
        # originates from outside of the cluster
        containerPort: 9094
        # sslClientAuth corresponds to the ssl.client.auth field from the Broker Configs in Kafka documentation
        # This defaults to be "required" for two-way SSL authentication when SSL is enabled, possible values are: "required", "requested", and "none"
        # sslClientAuth: "requested"
        # Config allows to specify ingress controller configuration per external listener
        config:
          # defaultIngressConfig describes which ingress configuration to use
          # when non set on the brokerIngressMapping field inside BrokerConfig
          defaultIngressConfig: "az2"
          # ingressConfig bundles the two available ingress configuration envoy and istio ingress
          ingressConfig:
            # Ingress config name should be unique per external listener
            ingress-az1:
              # In case of external listeners using LoadBalancer access method the value of this field is used to advertise the
              # Kafka broker external listener instead of the public IP of the provisioned LoadBalancer service (e.g. can be used to
              # advertise the listener using a URL recorded in DNS instead of public IP).
              # In case of external listeners using NodePort access method the broker instead of node public IP (see "brokerConfig.nodePortExternalIP")
              # is advertised on the address having the following format: <kafka-cluster-name>-<broker-id>.<namespace><value-specified-in-hostnameOverride-field>
              # hostnameOverride:
              # ServiceAnnotations defines annotations which will
              # be placed to the service or services created for the external listener
              # serviceAnnotations:
              # externalTrafficPolicy denotes if this Service desires to route external
              # traffic to node-local or cluster-wide endpoints. "Local" preserves the
              # client source IP and avoids a second hop for LoadBalancer and Nodeport
              # type services, but risks potentially imbalanced traffic spreading.
              # "Cluster" obscures the client source IP and may cause a second hop to
              # another node, but should have good overall load-spreading.
              # externalTrafficPolicy:
              # Service Type string describes ingress methods for a service
              # Only "NodePort" and "LoadBalancer" is supported.
              # Default value is LoadBalancer
              # serviceType:
              # envoyConfig defines the envoy specific config used for ingress-az1 external listener
              envoyConfig:
                #  replicas describes how many pods will be used for the created envoy proxy
                #  replicas: 1
                # image describes the Envoy docker image
                #  image: "banzaicloud/envoy:0.1.0"
                # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests
                # through this property
                # resourceRequirements:
                #  limits:
                #    memory: "300Mi"
                #    cpu: "200m"
                #  requests:
                #    memory: "300Mi"
                #    cpu: "200m"
                # serviceAccountName specifies the serviceAccount used for envoy
                # serviceAccountName: "envoy"
                # imagePullSecrets specifies the secret to use when using private registry
                # imagePullSecrets: "k8ssecret"
                # nodeSelector can be specified, which set the pod to fit on a node
                # nodeSelector:
                # tolerations can be specified, which set the pod's tolerations
                # tolerations:
                # annotations can be used to place annotation to the envoy created loadbalancer
                 annotations:
                   az1
                # loadBalancerSourceRanges refers to the k8s resource used in loadbalancer type services
                # loadBalancerSourceRanges:
            ingress-az1-istio:
              istioIngressConfig:
                # annotations can be used to place annotation to the envoy created loadbalancer
                annotations:
                  istio-az1
                # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests
                # through this property
                # resourceRequirements:
                #  limits:
                #    memory: "300Mi"
                #    cpu: "200m"
                #  requests:
                #    memory: "300Mi"
                #    cpu: "200m"
                #  replicas describes how many pods will be used for the created envoy proxy
                #  replicas: 1
                # nodeSelector can be specified, which set the pod to fit on a node
                # nodeSelector:
                # tolerations can be specified, which set the pod's tolerations
                # tolerations:
                # allows to set the created gateway configuration
                # gatewayConfig:
                # annotations will be placed on the created virtual service
                # virtualServiceAnnotations:
    # internalListeners specifies settings required to access kafka externally
    internalListeners:
      # type defines the used security type ssl, plaintext, sasl_plaintext, sasl_ssl
      - type: "ssl"
        # Kafka enables to name your listeners because of a bug only ssl and plaintext can be used
        name: "ssl"
        # containerPort describes what port should be used by the broker to handle the communication
        # originates from inside of the cluster
        containerPort: 29092
        # Support for multiple internal and external listeners are coming soon this value tells the operator which
        # internal listener configuration must be used for broker to broker communication
        usedForInnerBrokerCommunication: true
        # sslClientAuth corresponds to the ssl.client.auth field from the Broker Configs in Kafka documentation
        # This defaults to be "required" for two-way SSL authentication when SSL is enabled, possible values are: "required", "requested", and "none"
        # sslClientAuth: "requested"
    # sslSecrets contains information about ssl related kubernetes secrets if one of the
    # listener setting type set to ssl these fields must be populated too.
    sslSecrets:
      # tlsSecretName should contain all ssl certs required by kafka including: caCert, caKey, clientCert, clientKey
      # serverCert, serverKey, peerCert, peerKey
      tlsSecretName: "test-kafka-operator"
      # jksPasswordName should contain a password field which contains the jks password
      jksPasswordName: "test-kafka-operator-pass"
      # create tells the installed cert manager to create the required certs keys
      create: true
  # disruptionBudget defines the configuration for PodDisruptionBudget
  disruptionBudget:
  # create will enable the PodDisruptionBudget when set to true
    create: false
  # The budget to set for the PDB, can either be static number or a percentage
  #   budget: "1"
  # envoyConfig defines the envoy specific config used for externalListeners
  #envoyConfig:
  # replicas describes how many pods will be used for the created envoy proxy
  #  replicas: 1
  # image describes the Envoy docker image
  #  image: "banzaicloud/envoy:0.1.0"
  # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests
  # through this property
  #resourceRequirements:
  #  limits:
  #    memory: "300Mi"
  #    cpu: "200m"
  #  requests:
  #    memory: "300Mi"
  #    cpu: "200m"
  # serviceAccountName specifies the serviceAccount used for envoy
  #serviceAccountName: "envoy"
  # imagePullSecrets specifies the secret to use when using private registry
  #imagePullSecrets: "k8ssecret"
  # nodeSelector can be specified, which set the pod to fit on a node
  #nodeSelector:
  # tolerations can be specified, which set the pod's tolerations
  #tolerations:
  # annotations can be used to place annotation to the envoy created loadbalancer
  #annotations:
  # loadBalancerSourceRanges refers to the k8s resource used in loadbalancer type services
  #loadBalancerSourceRanges:
  # cruiseControlConfig describes the cruise control related configuration
  cruiseControlConfig:
    # image describes the CC docker image
    #image: "solsson/kafka-cruise-control@sha256:c70eae329b4ececba58e8cf4fa6e774dd2e0205988d8e5be1a70e622fcc46716"
    # CruiseControlEndpoint describes the endpoint where the already running CC is accessable. If set the Operator will not
    # try to install one
    #cruiseControlEndpoint: "localhost:8090"
    # resourceRequirements works exactly like Container resources, the user can specify the limit and the requests
    # through this property
    #resourceRequirements:
    #  limits:
    #    memory: "300Mi"
    #    cpu: "200m"
    #  requests:
    #    memory: "300Mi"
    #    cpu: "200m"
    # serviceAccountName specifies the serviceAccount used for cc
    #serviceAccountName: "envoy"
    # imagePullSecrets specifies the secret to use when using private registry
    #imagePullSecrets: "k8ssecret"
    # nodeSelector can be specified, which set the pod to fit on a node
    #nodeSelector:
    # tolerations can be specified, which set the pod's tolerations
    #tolerations:
    # Config describes the main configuration file called cruisecontrol.properties bootsrap.server and zookeeper.connect must left out
    # because those values are generated
    config: |
      # Copyright 2017 LinkedIn Corp. Licensed under the BSD 2-Clause License (the "License"). See License in the project root for license information.
      #
      # This is an example property file for Kafka Cruise Control. See KafkaCruiseControlConfig for more details.
      # Configuration for the metadata client.
      # =======================================
      # The maximum interval in milliseconds between two metadata refreshes.
      #metadata.max.age.ms=300000
      # Client id for the Cruise Control. It is used for the metadata client.
      #client.id=kafka-cruise-control
      # The size of TCP send buffer bytes for the metadata client.
      #send.buffer.bytes=131072
      # The size of TCP receive buffer size for the metadata client.
      #receive.buffer.bytes=131072
      # The time to wait before disconnect an idle TCP connection.
      #connections.max.idle.ms=540000
      # The time to wait before reconnect to a given host.
      #reconnect.backoff.ms=50
      # The time to wait for a response from a host after sending a request.
      #request.timeout.ms=30000
      # Configurations for the load monitor
      # =======================================
      # The number of metric fetcher thread to fetch metrics for the Kafka cluster
      num.metric.fetchers=1
      # The metric sampler class
      metric.sampler.class=com.linkedin.kafka.cruisecontrol.monitor.sampling.CruiseControlMetricsReporterSampler
      # Configurations for CruiseControlMetricsReporterSampler
      metric.reporter.topic.pattern=__CruiseControlMetrics
      # The sample store class name
      sample.store.class=com.linkedin.kafka.cruisecontrol.monitor.sampling.KafkaSampleStore
      # The config for the Kafka sample store to save the partition metric samples
      partition.metric.sample.store.topic=__KafkaCruiseControlPartitionMetricSamples
      # The config for the Kafka sample store to save the model training samples
      broker.metric.sample.store.topic=__KafkaCruiseControlModelTrainingSamples
      # The replication factor of Kafka metric sample store topic
      sample.store.topic.replication.factor=2
      # The config for the number of Kafka sample store consumer threads
      num.sample.loading.threads=8
      # The partition assignor class for the metric samplers
      metric.sampler.partition.assignor.class=com.linkedin.kafka.cruisecontrol.monitor.sampling.DefaultMetricSamplerPartitionAssignor
      # The metric sampling interval in milliseconds
      metric.sampling.interval.ms=120000
      metric.anomaly.detection.interval.ms=180000
      # The partition metrics window size in milliseconds
      partition.metrics.window.ms=300000
      # The number of partition metric windows to keep in memory
      num.partition.metrics.windows=1
      # The minimum partition metric samples required for a partition in each window
      min.samples.per.partition.metrics.window=1
      # The broker metrics window size in milliseconds
      broker.metrics.window.ms=300000
      # The number of broker metric windows to keep in memory
      num.broker.metrics.windows=20
      # The minimum broker metric samples required for a partition in each window
      min.samples.per.broker.metrics.window=1
      # The configuration for the BrokerCapacityConfigFileResolver (supports JBOD and non-JBOD broker capacities)
      capacity.config.file=config/capacity.json
      #capacity.config.file=config/capacityJBOD.json
      # Configurations for the analyzer
      # =======================================
      # The list of goals to optimize the Kafka cluster for with pre-computed proposals
      default.goals=com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.PotentialNwOutGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuUsageDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.TopicReplicaDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderBytesInDistributionGoal
      # The list of supported goals
      goals=com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.PotentialNwOutGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuUsageDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.TopicReplicaDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderBytesInDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.kafkaassigner.KafkaAssignerDiskUsageDistributionGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.PreferredLeaderElectionGoal
      # The list of supported hard goals
      hard.goals=com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuCapacityGoal
      # The minimum percentage of well monitored partitions out of all the partitions
      min.monitored.partition.percentage=0.95
      # The balance threshold for CPU
      cpu.balance.threshold=1.1
      # The balance threshold for disk
      disk.balance.threshold=1.1
      # The balance threshold for network inbound utilization
      network.inbound.balance.threshold=1.1
      # The balance threshold for network outbound utilization
      network.outbound.balance.threshold=1.1
      # The balance threshold for the replica count
      replica.count.balance.threshold=1.1
      # The capacity threshold for CPU in percentage
      cpu.capacity.threshold=0.8
      # The capacity threshold for disk in percentage
      disk.capacity.threshold=0.8
      # The capacity threshold for network inbound utilization in percentage
      network.inbound.capacity.threshold=0.8
      # The capacity threshold for network outbound utilization in percentage
      network.outbound.capacity.threshold=0.8
      # The threshold to define the cluster to be in a low CPU utilization state
      cpu.low.utilization.threshold=0.0
      # The threshold to define the cluster to be in a low disk utilization state
      disk.low.utilization.threshold=0.0
      # The threshold to define the cluster to be in a low network inbound utilization state
      network.inbound.low.utilization.threshold=0.0
      # The threshold to define the cluster to be in a low disk utilization state
      network.outbound.low.utilization.threshold=0.0
      # The metric anomaly percentile upper threshold
      metric.anomaly.percentile.upper.threshold=90.0
      # The metric anomaly percentile lower threshold
      metric.anomaly.percentile.lower.threshold=10.0
      # How often should the cached proposal be expired and recalculated if necessary
      proposal.expiration.ms=60000
      # The maximum number of replicas that can reside on a broker at any given time.
      max.replicas.per.broker=10000
      # The number of threads to use for proposal candidate precomputing.
      num.proposal.precompute.threads=1
      # the topics that should be excluded from the partition movement.
      #topics.excluded.from.partition.movement
      # Configurations for the executor
      # =======================================
      # The max number of partitions to move in/out on a given broker at a given time.
      num.concurrent.partition.movements.per.broker=10
      # The interval between two execution progress checks.
      execution.progress.check.interval.ms=10000
      # Configurations for anomaly detector
      # =======================================
      # The goal violation notifier class
      anomaly.notifier.class=com.linkedin.kafka.cruisecontrol.detector.notifier.SelfHealingNotifier
      # The metric anomaly finder class
      metric.anomaly.finder.class=com.linkedin.kafka.cruisecontrol.detector.KafkaMetricAnomalyFinder
      # The anomaly detection interval
      anomaly.detection.interval.ms=10000
      # The goal violation to detect.
      anomaly.detection.goals=com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal,com.linkedin.kafka.cruisecontrol.analyzer.goals.CpuCapacityGoal
      # The interested metrics for metric anomaly analyzer.
      metric.anomaly.analyzer.metrics=BROKER_PRODUCE_LOCAL_TIME_MS_MAX,BROKER_PRODUCE_LOCAL_TIME_MS_MEAN,BROKER_CONSUMER_FETCH_LOCAL_TIME_MS_MAX,BROKER_CONSUMER_FETCH_LOCAL_TIME_MS_MEAN,BROKER_FOLLOWER_FETCH_LOCAL_TIME_MS_MAX,BROKER_FOLLOWER_FETCH_LOCAL_TIME_MS_MEAN,BROKER_LOG_FLUSH_TIME_MS_MAX,BROKER_LOG_FLUSH_TIME_MS_MEAN
      ## Adjust accordingly if your metrics reporter is an older version and does not produce these metrics.
      #metric.anomaly.analyzer.metrics=BROKER_PRODUCE_LOCAL_TIME_MS_50TH,BROKER_PRODUCE_LOCAL_TIME_MS_999TH,BROKER_CONSUMER_FETCH_LOCAL_TIME_MS_50TH,BROKER_CONSUMER_FETCH_LOCAL_TIME_MS_999TH,BROKER_FOLLOWER_FETCH_LOCAL_TIME_MS_50TH,BROKER_FOLLOWER_FETCH_LOCAL_TIME_MS_999TH,BROKER_LOG_FLUSH_TIME_MS_50TH,BROKER_LOG_FLUSH_TIME_MS_999TH
      # The zk path to store failed broker information.
      failed.brokers.zk.path=/CruiseControlBrokerList
      # Topic config provider class
      topic.config.provider.class=com.linkedin.kafka.cruisecontrol.config.KafkaTopicConfigProvider
      # The cluster configurations for the KafkaTopicConfigProvider
      cluster.configs.file=config/clusterConfigs.json
      # The maximum time in milliseconds to store the response and access details of a completed user task.
      completed.user.task.retention.time.ms=21600000
      # The maximum time in milliseconds to retain the demotion history of brokers.
      demotion.history.retention.time.ms=86400000
      # The maximum number of completed user tasks for which the response and access details will be cached.
      max.cached.completed.user.tasks=100
      # The maximum number of user tasks for concurrently running in async endpoints across all users.
      max.active.user.tasks=25
      # Enable self healing for all anomaly detectors, unless the particular anomaly detector is explicitly disabled
      self.healing.enabled=true
      # Enable self healing for broker failure detector
      #self.healing.broker.failure.enabled=true
      # Enable self healing for goal violation detector
      #self.healing.goal.violation.enabled=true
      # Enable self healing for metric anomaly detector
      #self.healing.metric.anomaly.enabled=true
      # configurations for the webserver
      # ================================
      # HTTP listen port
      webserver.http.port=9090
      # HTTP listen address
      webserver.http.address=0.0.0.0
      # Whether CORS support is enabled for API or not
      webserver.http.cors.enabled=false
      # Value for Access-Control-Allow-Origin
      webserver.http.cors.origin=http://localhost:8080/
      # Value for Access-Control-Request-Method
      webserver.http.cors.allowmethods=OPTIONS,GET,POST
      # Headers that should be exposed to the Browser (Webapp)
      # This is a special header that is used by the
      # User Tasks subsystem and should be explicitly
      # Enabled when CORS mode is used as part of the
      # Admin Interface
      webserver.http.cors.exposeheaders=User-Task-ID
      # REST API default prefix
      # (dont forget the ending *)
      webserver.api.urlprefix=/kafkacruisecontrol/*
      # Location where the Cruise Control frontend is deployed
      webserver.ui.diskpath=./cruise-control-ui/dist/
      # URL path prefix for UI
      # (dont forget the ending *)
      webserver.ui.urlprefix=/*
      # Time After which request is converted to Async
      webserver.request.maxBlockTimeMs=10000
      # Default Session Expiry Period
      webserver.session.maxExpiryTimeMs=60000
      # Session cookie path
      webserver.session.path=/
      # Server Access Logs
      webserver.accesslog.enabled=true
      # Location of HTTP Request Logs
      webserver.accesslog.path=access.log
      # HTTP Request Log retention days
      webserver.accesslog.retention.days=14
    # CapacityConfig describes Cruise Control config capacity.json if unset the operator will calculate it for the cluster
    capacityConfig: |
      {
        "brokerCapacities":[
          {
            "brokerId": "-1",
            "capacity": {
              "DISK": "10000",
              "CPU": "100",
              "NW_IN": "10000",
              "NW_OUT": "10000"
            },
            "doc": "This is the default capacity. Capacity unit used for disk is in MB, cpu is in percentage, network throughput is in KB."
          }
        ]
      }
    # ClusterConfigs describes Cruise Control config clusterConfigs.json
    clusterConfig: |
      {
        "min.insync.replicas": 2
      }
  # monitoringConfig describes the monitoring related configs
  #monitoringConfig:
  # jmxImage describes the used prometheus jmx exporter agent container
  #  jmxImage: "ghcr.io/banzaicloud/jmx-javaagent:0.16.1"
  # pathToJar describes the path to the jar file in the given image
  #  pathToJar: "/opt/jmx_exporter/jmx_prometheus_javaagent-0.16.1.jar"
  # kafkaJMXExporterConfig describes jmx exporter config for Kafka
  #  kafkaJMXExporterConfig: |
  #   lowercaseOutputName: true
  # cCJMXExporterConfig describes jmx exporter config for CruiseControl
  # cCJMXExporterConfig: |
  #  lowercaseOutputName: true
